{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e53172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3180e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s: i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e615bc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "# split the dataset\n",
    "import random \n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xval, Yval = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9434a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/(n_embd * n_hidden)**0.5 ##* 0.01 # we scale by 0.01 to make it smaller and therefore not kill the tanh activation at the start, because if the inputs are too large, tanh will saturate and gradients will be very small\n",
    "# b1 = torch.randn(n_hidden, generator=g) * 0.01\n",
    "# we dont need bias when using batch norm because batch norm has its own bias term\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.01 # we scale by 0.01 to make the initial logits small, so that softmax is evenly distributed at the start\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0\n",
    "\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, W2, b2 , bngain, bnbias]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9250cc38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 200])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpreact.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f91aceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpreact.mean(0,keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ff54b3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3111\n",
      "  10000/ 200000: 2.5608\n",
      "  20000/ 200000: 1.8233\n",
      "  30000/ 200000: 2.3316\n",
      "  40000/ 200000: 2.2627\n",
      "  50000/ 200000: 2.0380\n",
      "  60000/ 200000: 1.8556\n",
      "  70000/ 200000: 1.9904\n",
      "  80000/ 200000: 2.3875\n",
      "  90000/ 200000: 2.2780\n",
      " 100000/ 200000: 1.9153\n",
      " 110000/ 200000: 2.0341\n",
      " 120000/ 200000: 2.0200\n",
      " 130000/ 200000: 2.1637\n",
      " 140000/ 200000: 1.7200\n",
      " 150000/ 200000: 1.9482\n",
      " 160000/ 200000: 2.2946\n",
      " 170000/ 200000: 2.4649\n",
      " 180000/ 200000: 2.2373\n",
      " 190000/ 200000: 2.1730\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for step in range(max_steps):\n",
    "\n",
    "    # MINIBATCH SAMPLE\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # FORWARD PASS\n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "\n",
    "    # Linear layer\n",
    "    hpreact = embcat @ W1 #+ b1\n",
    "\n",
    "    #Batch Normalization Layer\n",
    "    # We do batch normalization on the hidden layer pre-activation, because otherwise tanh can saturate and kill gradients\n",
    "    # we use the trainable parameters bngain and bnbias to allow the network to learn the optimal scale and shift after normalization\n",
    "    bmeani = hpreact.mean(0, keepdim=True)\n",
    "    bnstdi = hpreact.std(0, keepdim=True)\n",
    "    hpreact = bngain * (hpreact - bmeani)/bnstdi + bnbias\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # update running mean and std\n",
    "        bnmean_running = 0.9 * bnmean_running + 0.1 * bmeani\n",
    "        bnstd_running = 0.9 * bnstd_running + 0.1 * bnstdi\n",
    "\n",
    "\n",
    "    # Non-linearity layer / activation function\n",
    "    h = torch.tanh(hpreact)\n",
    "\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "    # BACKWARD PASS\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # SGD step\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad\n",
    "\n",
    "    lossi.append(loss.item())\n",
    "\n",
    "    if step % 10000 == 0:\n",
    "        print(f\"{step:7d}/{max_steps:7d}: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "54123f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate batch norm statistics by doing a forward pass through the training set\n",
    "# but we are now doing this in the training loop\n",
    "with torch.no_grad():\n",
    "    #pass the entire training set\n",
    "    emb = C[Xtr]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1\n",
    "    # compute the mean and std over the entire training set\n",
    "    bnmean_running = hpreact.mean(0, keepdim=True)\n",
    "    bnstd_running = hpreact.std(0, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d419430e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 2.1162\n",
      "val loss: 2.1822\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x,y = {'train': (Xtr, Ytr), 'val': (Xval, Yval), 'test': (Xte, Yte)}[split]\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1\n",
    "    #hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True))/hpreact.std(0, keepdim=True) + bnbias\n",
    "    hpreact = bngain * (hpreact - bnmean_running)/bnstd_running + bnbias\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(f\"{split} loss: {loss.item():.4f}\")\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "96d282b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cexi.\n",
      "karalynn.\n",
      "ima.\n",
      "kayden.\n",
      "mariyanainella.\n",
      "kamandr.\n",
      "samiyah.\n",
      "jaxsi.\n",
      "gotti.\n",
      "mckiellah.\n",
      "jayla.\n",
      "daria.\n",
      "emilyssaly.\n",
      "tiaviyah.\n",
      "fobstihilinghviah.\n",
      "asu.\n",
      "jadri.\n",
      "antil.\n",
      "gyan.\n",
      "ivan.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        embcat = emb.view(1, -1)\n",
    "        hpreact = embcat @ W1  #+ b1\n",
    "        hpreact = bngain * (hpreact - bnmean_running)/bnstd_running + bnbias\n",
    "        h = torch.tanh(hpreact)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(itos[i] for i in out))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d61c8b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.2958)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(1/27.0).log()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
