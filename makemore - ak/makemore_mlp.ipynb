{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf5ee6f1",
   "metadata": {},
   "source": [
    "## Going to train this NN\n",
    "```mermaid\n",
    "graph TD\n",
    "    IN[Input context 3 chars]\n",
    "\n",
    "    subgraph Embedding_Layer_C\n",
    "        EMB[Embedding C 27x2 to 3x2]\n",
    "    end\n",
    "\n",
    "    subgraph Flatten\n",
    "        FLAT[Flatten to vector size 6]\n",
    "    end\n",
    "\n",
    "    subgraph Hidden_Layer\n",
    "        H[Hidden layer 300 neurons]\n",
    "    end\n",
    "\n",
    "    subgraph Output_Layer\n",
    "        LOGITS[Output logits size 27]\n",
    "    end\n",
    "\n",
    "    subgraph Softmax\n",
    "        PROBS[Softmax to probabilities]\n",
    "    end\n",
    "\n",
    "    subgraph Prediction\n",
    "        PRED[Pick next character]\n",
    "    end\n",
    "\n",
    "    IN --> EMB\n",
    "    EMB --> FLAT\n",
    "    FLAT --> H\n",
    "    H --> LOGITS\n",
    "    LOGITS --> PROBS\n",
    "    PROBS --> PRED\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c673b1de",
   "metadata": {},
   "source": [
    "#### Example flow\n",
    "\n",
    "```mermaid\n",
    "\n",
    "graph TD\n",
    "    A0[\"Start: context of 3 chars<br/>example: ., ., e\"]\n",
    "    A1[\"Embed each char using C<br/>C shape 27 x 2 (one row per char incl. '.')\"]\n",
    "    A2[\"Stacked embeddings<br/>shape 3 x 2\"]\n",
    "    A3[\"Flatten<br/>shape 6<br/>order: c1_d1 c1_d2 c2_d1 c2_d2 c3_d1 c3_d2\"]\n",
    "    A4[\"Hidden layer<br/>Linear 6→300 then tanh\"]\n",
    "    A5[\"Output layer<br/>Linear 300→27 logits\"]\n",
    "    A6[\"Softmax<br/>probabilities over 27 chars\"]\n",
    "    A7[\"Pick next char<br/>(sample or argmax)\"]\n",
    "\n",
    "    A0 --> A1 --> A2 --> A3 --> A4 --> A5 --> A6 --> A7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0dcacbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2980c73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines() \n",
    "words[:8]  # show first 8 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15be90f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)  # number of words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c1a11f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))  # all unique characters in the dataset\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}  # char to int\n",
    "stoi['.'] = 0  # add a special 'end of word' character\n",
    "itos = {i:s for s,i in stoi.items()}  # int to char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8620453",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3  # context length: how many characters do we take to predict the next one?, bigram, trigram, etc.\n",
    "\n",
    "X, Y = [], []\n",
    "for w in words:\n",
    "    context = [0] * block_size  # initialize with 'end of word' characters\n",
    "    for ch in w + '.':  # for each character plus the 'end of word' character\n",
    "        ix = stoi[ch]  # get the integer representation\n",
    "        X.append(context)  # add the current context to inputs\n",
    "        Y.append(ix)  # add the target character to outputs\n",
    "        context = context[1:] + [ix]  # slide the context window / rolling window\n",
    "\n",
    "# So for \"emma\", we will have:\n",
    "# X: [.,.,.] -> Y: 'e'\n",
    "# X: [.,.,e] -> Y: 'm'\n",
    "# X: [.,e,m] -> Y: 'm'\n",
    "# X: [e,m,m] -> Y: 'a'\n",
    "# X: [m,m,a] -> Y: '.' \n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af3976fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 3])\n",
      "torch.Size([228146])\n"
     ]
    }
   ],
   "source": [
    "print(X.shape) # (228146, 3) 228146 rows, where each row has 3 context characters, so the 3 characters used to predict the next character which is in Y\n",
    "print(Y.shape) # (228146,) 228146 target characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d00a5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6328, -0.1950])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = torch.randn((27, 2))  # character embedding matrix, 27 characters, each represented by a 2-dimensional vector, so each character/it's integer representation is mapped to a point in 2D space\n",
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa6cc5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes=27).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f716b28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6328, -0.1950])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes=27).float() @ C  # get the embedding for character 5 by multiplying one-hot vector with embedding matrix C\n",
    "# the reason we get the same result as C[5] is because when we multiply the one-hot vector with C, only the row corresponding to the '1' in the one-hot vector contributes to the result, effectively selecting that row from C.\n",
    "# we take a (1, 27) @ (27, 2) -> (1, 2) vector\n",
    "\n",
    "# so we could send in a 27-dimensional one-hot vector to get the embedding for any character in the first layer of our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f1d5edc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 3, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape # (228146, 3, 2) we have 228146 examples of contexts, each with a context of 3 characters, each character represented by a 2-dimensional embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2f9d0347",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((6, 300))  # first layer weights, input dim is 6 (3 characters * 2 dimensions each), output dim is 100\n",
    "b1 = torch.randn(300)  # first layer bias, output dim is 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "988202f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (684438x2 and 6x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43memb\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m + b1  \u001b[38;5;66;03m# doesnt work because the shapes are not aligned for matrix multiplication\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (684438x2 and 6x100)"
     ]
    }
   ],
   "source": [
    "emb @ W1 + b1  # doesnt work because the shapes are not aligned for matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f39836ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9260,  0.4435,  0.9260,  0.4435,  0.9260,  0.4435],\n",
       "        [ 0.9260,  0.4435,  0.9260,  0.4435, -0.6328, -0.1950],\n",
       "        [ 0.9260,  0.4435, -0.6328, -0.1950, -0.0846,  0.2794],\n",
       "        ...,\n",
       "        [ 1.3785,  0.5128,  1.3785,  0.5128,  0.5580, -0.7022],\n",
       "        [ 1.3785,  0.5128,  0.5580, -0.7022,  1.3785,  0.5128],\n",
       "        [ 0.5580, -0.7022,  1.3785,  0.5128,  1.2880,  0.1033]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([emb[:,0, :], emb[:,1, :], emb[:,2, :]],1 ) # get the embeddings for each of the 3 context characters separately\n",
    "# so basically in simple terms we are flattening the (3, 2) embedding for each example into a (6,) vector by concatenating the embeddings of the 3 characters together\n",
    "# so one example could be [c1_dim1, c1_dim2, c2_dim1, c2_dim2, c3_dim1, c3_dim2] instead of [[c1_dim1, c1_dim2], [c2_dim1, c2_dim2], [c3_dim1, c3_dim2]]\n",
    "\n",
    "# this is ugly if we want to scale to larger context sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "82fa1c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9260,  0.4435,  0.9260,  0.4435,  0.9260,  0.4435],\n",
       "        [ 0.9260,  0.4435,  0.9260,  0.4435, -0.6328, -0.1950],\n",
       "        [ 0.9260,  0.4435, -0.6328, -0.1950, -0.0846,  0.2794],\n",
       "        ...,\n",
       "        [ 1.3785,  0.5128,  1.3785,  0.5128,  0.5580, -0.7022],\n",
       "        [ 1.3785,  0.5128,  0.5580, -0.7022,  1.3785,  0.5128],\n",
       "        [ 0.5580, -0.7022,  1.3785,  0.5128,  1.2880,  0.1033]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unbind(emb,1)  # is equivalent to this list [emb[:,0, :], emb[:,1, :], emb[:,2, :]]\n",
    "\n",
    "torch.cat(torch.unbind(emb,1),1)  # more scalable way to do the same thing as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "015e5b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9260,  0.4435,  0.9260,  0.4435,  0.9260,  0.4435],\n",
       "        [ 0.9260,  0.4435,  0.9260,  0.4435, -0.6328, -0.1950],\n",
       "        [ 0.9260,  0.4435, -0.6328, -0.1950, -0.0846,  0.2794],\n",
       "        ...,\n",
       "        [ 1.3785,  0.5128,  1.3785,  0.5128,  0.5580, -0.7022],\n",
       "        [ 1.3785,  0.5128,  0.5580, -0.7022,  1.3785,  0.5128],\n",
       "        [ 0.5580, -0.7022,  1.3785,  0.5128,  1.2880,  0.1033]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# even simpler way is to use view\n",
    "emb.reshape(emb.shape[0],6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2944cd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 300])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.tanh(emb.reshape(len(emb),6) @ W1 + b1)  # now this works, we have (228146, 6) @ (6, 100) -> (228146, 100)\n",
    "h.shape  # (228146, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bd3264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d5f3ccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((300, 27))  # second layer weights, input dim is 100, output dim is 27 (number of characters)\n",
    "b2 = torch.randn(27)  # second layer bias, output dim is 27\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "61f98fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 27])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = h @ W2 + b2  # (228146, 100) @ (100, 27) -> (228146, 27)\n",
    "#Softmax to get probabilities\n",
    "counts = logits.exp()  # (228146, 27)\n",
    "probs = counts / counts.sum(1, keepdim=True)  # (228146, 27) \n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a207d28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(23.6600, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -probs[torch.arange(len(Y)), Y].log().mean() # probs is (228146, 27), Y is (228146,)\n",
    "# we use torch.arange(len(Y)) to create a tensor of indices from 0 to 228145, which corresponds to the row indices in probs\n",
    "# Y contains the target character indices for each example, which we use as the column indices in probs\n",
    "# so probs[torch.arange(len(Y)), Y] extracts the predicted probabilities for the correct target characters for all examples\n",
    "# we then take the logarithm of these probabilities, negate them to get the negative log-likelihood, and finally compute the mean to get the average loss across all examples.\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f34c4304",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in [C, W1, b1, W2, b2]:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e12230ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.274520874023438\n",
      "14.87217903137207\n",
      "14.08586311340332\n",
      "10.791343688964844\n",
      "9.548832893371582\n",
      "11.837512016296387\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m [C, W1, b1, W2, b2]:\n\u001b[32m     10\u001b[39m     p.grad = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# set gradients to zero\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# compute gradients\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m#update\u001b[39;00m\n\u001b[32m     14\u001b[39m lr = \u001b[32m0.1\u001b[39m  \u001b[38;5;66;03m# learning rate\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for _ in range(1000):    \n",
    "    #Forward pass\n",
    "    emb = C[X]  # (228146, 3, 2)\n",
    "    h = torch.tanh(emb.reshape(len(emb),6) @ W1 + b1)  # (228146, 100)\n",
    "    logits = h @ W2 + b2  # (228146, 27)\n",
    "    loss = F.cross_entropy(logits, Y)  # this does the same as the loss calculation above in a more numerically stable way\n",
    "    print(loss.item())\n",
    "    #backward pass\n",
    "    for p in [C, W1, b1, W2, b2]:\n",
    "        p.grad = None  # set gradients to zero\n",
    "    loss.backward()  # compute gradients\n",
    "\n",
    "    #update\n",
    "    lr = 0.1  # learning rate\n",
    "    for p in [C, W1, b1, W2, b2]:\n",
    "        p.data += -lr * p.grad  # gradient descent step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d1a00048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minibatch training, should do train/val/test but skipping for now\n",
    "batch_size = 32\n",
    "for _ in range(100000):\n",
    "    #Mini-batch sample\n",
    "    ix = torch.randint(0, X.shape[0], (batch_size,))  # random indices for the batch\n",
    "    Xb, Yb = X[ix], Y[ix]  # minibatch of inputs and targets\n",
    "\n",
    "    #Forward pass\n",
    "    emb = C[Xb]  # (32, 3, 2)\n",
    "    h = torch.tanh(emb.reshape(len(emb),6) @ W1 + b1)  # (32, 100)\n",
    "    logits = h @ W2 + b2  # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Yb)  # cross-entropy loss for the minibatch\n",
    "    \n",
    "    #backward pass\n",
    "    for p in [C, W1, b1, W2, b2]:\n",
    "        p.grad = None  # set gradients to zero\n",
    "    loss.backward()  # compute gradients\n",
    "\n",
    "    #update\n",
    "    lr = 0.2  # learning rate\n",
    "    for p in [C, W1, b1, W2, b2]:\n",
    "        p.data += -lr * p.grad  # gradient descent step\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "be51643a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.383111000061035\n"
     ]
    }
   ],
   "source": [
    "emb = C[X]  # (32, 3, 2)\n",
    "h = torch.tanh(emb.reshape(len(emb),6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Y)  # cross-entropy loss for the minibatch\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "958e5a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lux\n",
      "niyah\n",
      "jel\n",
      "joa\n",
      "lorah\n",
      "khustyn\n",
      "lei\n",
      "ele\n",
      "caratvin\n",
      "juf\n",
      "kyla\n",
      "jouleigus\n",
      "xhrus\n",
      "joh\n",
      "lolyn\n",
      "reyah\n",
      "aton\n",
      "ilei\n",
      "ubry\n",
      "kyna\n"
     ]
    }
   ],
   "source": [
    "# sampling from the model\n",
    "for _ in range(20):  # generate 20 names\n",
    "    out = []\n",
    "    context = [0] * block_size  # start with 'end of word' characters\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]  # (1, 3, 2)\n",
    "        h = torch.tanh(emb.reshape(1,6) @ W1 + b1)  # (1, 100)\n",
    "        logits = h @ W2 + b2  # (1, 27)\n",
    "        probs = F.softmax(logits, dim=1)  # (1, 27)\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()  # sample from the distribution\n",
    "        if ix == 0:  # 'end of word' character\n",
    "            break\n",
    "        out.append(itos[ix])\n",
    "        context = context[1:] + [ix]  # slide the context window\n",
    "    print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
