{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d68b842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/hojjatk/mnist-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22.0M/22.0M [00:03<00:00, 7.21MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Path to dataset files: /Users/Marius/.cache/kagglehub/datasets/hojjatk/mnist-dataset/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"hojjatk/mnist-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52be351a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28]) torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import struct\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "root = Path.home() / \".cache/kagglehub/datasets/hojjatk/mnist-dataset/versions/1\"\n",
    "\n",
    "def load_idx_images(path):\n",
    "    with path.open(\"rb\") as f:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        data = np.frombuffer(f.read(), dtype=np.uint8).reshape(num,\n",
    "    rows, cols)\n",
    "    return data\n",
    "\n",
    "def load_idx_labels(path):\n",
    "    with path.open(\"rb\") as f:\n",
    "        magic, num = struct.unpack(\">II\", f.read(8))\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "X_train = load_idx_images(root / \"train-images.idx3-ubyte\")\n",
    "y_train = load_idx_labels(root / \"train-labels.idx1-ubyte\")\n",
    "X_test  = load_idx_images(root / \"t10k-images.idx3-ubyte\")\n",
    "y_test  = load_idx_labels(root / \"t10k-labels.idx1-ubyte\")\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32) / 255.0\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32) / 255.0\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16ad486e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3141\n",
      "Epoch 2, Loss: 0.2593\n",
      "Epoch 3, Loss: 0.2277\n",
      "Epoch 4, Loss: 0.2099\n",
      "Epoch 5, Loss: 0.1961\n",
      "Epoch 6, Loss: 0.1841\n",
      "Epoch 7, Loss: 0.1735\n",
      "Epoch 8, Loss: 0.1651\n",
      "Epoch 9, Loss: 0.1565\n",
      "Epoch 10, Loss: 0.1477\n",
      "Test Loss: 0.1115, Test Accuracy: 96.40%\n"
     ]
    }
   ],
   "source": [
    "class MLPScratch(nn.Module):\n",
    "    def __init__(self, input_size, num_outputs, num_hiddens,\n",
    "                 lr=0.1, sigma=0.1, batch_size=256, num_epochs=10):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.W1 = nn.Parameter(torch.randn(input_size, num_hiddens) * sigma)\n",
    "        self.b1 = nn.Parameter(torch.zeros(num_hiddens))\n",
    "        self.W2 = nn.Parameter(torch.randn(num_hiddens, 128) * sigma)\n",
    "        self.b2 = nn.Parameter(torch.zeros(128))\n",
    "        self.W3 = nn.Parameter(torch.randn(128, num_outputs) * sigma)\n",
    "        self.b3 = nn.Parameter(torch.zeros(num_outputs))\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "    \n",
    "    def relu(self,x):\n",
    "        a = torch.zeros_like(x)\n",
    "        return torch.clamp(x,min=a)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        X = x.view(-1,self.input_size) ## flatten from (batch_size, 28, 28) to (batch_size, 784)\n",
    "        h1 = self.relu(X @ self.W1 + self.b1)\n",
    "        h = self.relu(h1 @ self.W2 + self.b2)\n",
    "        return h @ self.W3 + self.b3\n",
    "\n",
    "model = MLPScratch(input_size=28*28, num_outputs=10, num_hiddens=256)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "## optimizer takes parameters to optimize\n",
    "optimizer = torch.optim.SGD([model.W1, model.b1, model.W2, model.b2, model.W3, model.b3], lr=model.lr)\n",
    "num_batches = X_train.shape[0] // model.batch_size\n",
    "for epoch in range(model.num_epochs):\n",
    "    for i in range(num_batches):\n",
    "        X_batch = X_train[i*model.batch_size:(i+1)*model.batch_size]\n",
    "        y_batch = y_train[i*model.batch_size:(i+1)*model.batch_size]\n",
    "        \n",
    "        logits = model.forward(X_batch)\n",
    "        loss = loss_fn(logits, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "with torch.no_grad():\n",
    "    test_logits = model.forward(X_test)\n",
    "    test_loss = loss_fn(test_logits, y_test)\n",
    "    predictions = torch.argmax(test_logits, dim=1)\n",
    "    accuracy = (predictions == y_test).float().mean()\n",
    "    print(f\"Test Loss: {test_loss.item():.4f}, Test Accuracy: {accuracy.item()*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d6836b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3585\n",
      "Epoch 2, Loss: 0.3285\n",
      "Epoch 3, Loss: 0.2978\n",
      "Epoch 4, Loss: 0.2716\n",
      "Epoch 5, Loss: 0.2481\n",
      "Epoch 6, Loss: 0.2296\n",
      "Epoch 7, Loss: 0.2147\n",
      "Epoch 8, Loss: 0.2027\n",
      "Epoch 9, Loss: 0.1910\n",
      "Epoch 10, Loss: 0.1804\n",
      "Test Loss: 0.1217, Test Accuracy: 96.46%\n"
     ]
    }
   ],
   "source": [
    "## With high level API\n",
    "class MLPHighLevel(nn.Module):\n",
    "    def __init__(self, input_size, num_outputs, num_hiddens):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1, end_dim=-1), # flatten input, from (batch_size, 28, 28) to (batch_size, 784)\n",
    "            nn.Linear(input_size, num_hiddens), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hiddens, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_outputs)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "model_hl = MLPHighLevel(input_size=28*28, num_outputs=10, num_hiddens=256)\n",
    "loss_fn_hl = nn.CrossEntropyLoss()\n",
    "optimizer_hl = torch.optim.SGD(model_hl.parameters(), lr=0.1)\n",
    "num_epochs_hl = 10\n",
    "batch_size_hl = 256\n",
    "num_batches_hl = X_train.shape[0] // batch_size_hl\n",
    "for epoch in range(num_epochs_hl):\n",
    "    for i in range(num_batches_hl):\n",
    "        X_batch = X_train[i*batch_size_hl:(i+1)*batch_size_hl]\n",
    "        y_batch = y_train[i*batch_size_hl:(i+1)*batch_size_hl]\n",
    "        \n",
    "        logits = model_hl.forward(X_batch)\n",
    "        loss = loss_fn_hl(logits, y_batch)\n",
    "        \n",
    "        optimizer_hl.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_hl.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "with torch.no_grad():\n",
    "    test_logits_hl = model_hl.forward(X_test)\n",
    "    test_loss_hl = loss_fn_hl(test_logits_hl, y_test)\n",
    "    predictions_hl = torch.argmax(test_logits_hl, dim=1)\n",
    "    accuracy_hl = (predictions_hl == y_test).float().mean()\n",
    "    print(f\"Test Loss: {test_loss_hl.item():.4f}, Test Accuracy: {accuracy_hl.item()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98e85693",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1,1,28,28) # reshape to add channel dimension\n",
    "# channel dimension is 1 for grayscale images, it's needed for Conv2d layers\n",
    "# because Conv2d expects input shape (batch_size, channels, height, width)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd1e826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNScratch(nn.Module):\n",
    "    def __init__(self, lr=0.1, sigma=0.1, batch_size=256, num_epochs=10):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        # Conv1: in_channels=1, out_channels=16, kernel=3x3\n",
    "        # basically 16 filters of size 1x3x3, each filter creates one output channel/activation map\n",
    "        # which has to be equal to the number of input channels (1 for grayscale images)\n",
    "        self.Wc1 = nn.Parameter(torch.randn(16, 1, 3, 3) * sigma)\n",
    "        self.bc1 = nn.Parameter(torch.zeros(16))\n",
    "        # Conv2: in_channels=16, out_channels=32, kernel=3x3\n",
    "        # basically 32 filters of size 16x3x3, each filter creates one output channel/activation map\n",
    "        # each of the 32 filters looks at all the 16 activation maps/channels from previous layer\n",
    "        self.Wc2 = nn.Parameter(torch.randn(32, 16, 3, 3) * sigma)\n",
    "        self.bc2 = nn.Parameter(torch.zeros(32))\n",
    "        \n",
    "        # With padding=1: 28->28 after each conv\n",
    "        # After maxpool 2x2 (stride 2): 28->14\n",
    "        flat = 32 * 14 * 14  # 32 channels, 14x14 feature map size\n",
    "\n",
    "        # Final linear: flat -> 10\n",
    "        self.W = nn.Parameter(torch.randn(flat, 10) * sigma)\n",
    "        self.b = nn.Parameter(torch.zeros(10))\n",
    "\n",
    "    def relu(self, x):\n",
    "        return torch.clamp(x, min=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x should be (B, 1, 28, 28)\n",
    "        h = F.conv2d(x, self.Wc1, self.bc1, stride=1, padding=1)     # (B,16,28,28)\n",
    "        h = self.relu(h)\n",
    "        h = F.conv2d(h, self.Wc2, self.bc2, stride=1, padding=1)     # (B,32,28,28)\n",
    "        h = self.relu(h)\n",
    "        h = F.max_pool2d(h, kernel_size=2, stride=2)  # (B,32,14,14), scales down by factor 2\n",
    "        h = h.view(h.shape[0], -1)             # (B, 32*14*14), flatten\n",
    "        logits = h @ self.W + self.b           # (B,10)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ecea81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_cnn = CNNScratch()\n",
    "logits = model_cnn.forward(X_train[:4])  # pass a small batch to see shapes\n",
    "print(\"logits\", logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "481fb655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3376\n",
      "Epoch 2, Loss: 0.2997\n",
      "Epoch 3, Loss: 0.2776\n",
      "Epoch 4, Loss: 0.2524\n",
      "Epoch 5, Loss: 0.2338\n",
      "Epoch 6, Loss: 0.2154\n",
      "Epoch 7, Loss: 0.2025\n",
      "Epoch 8, Loss: 0.1892\n",
      "Epoch 9, Loss: 0.1768\n",
      "Epoch 10, Loss: 0.1657\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_cnn.parameters(), lr=model_cnn.lr)\n",
    "num_batches = X_train.shape[0] // model_cnn.batch_size\n",
    "for epoch in range(model_cnn.num_epochs):\n",
    "    for i in range(num_batches):\n",
    "        X_batch = X_train[i*model_cnn.batch_size:(i+1)*model_cnn.batch_size]\n",
    "        y_batch = y_train[i*model_cnn.batch_size:(i+1)*model_cnn.batch_size]\n",
    "        \n",
    "        logits = model_cnn.forward(X_batch)\n",
    "        loss = loss_fn(logits, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "956f7016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0582, Test Accuracy: 98.09%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "with torch.no_grad():\n",
    "    X_test = X_test.reshape(-1,1,28,28)  # reshape test set\n",
    "    test_logits = model_cnn.forward(X_test)\n",
    "    test_loss = loss_fn(test_logits, y_test)\n",
    "    predictions = torch.argmax(test_logits, dim=1)\n",
    "    accuracy = (predictions == y_test).float().mean()\n",
    "    print(f\"Test Loss: {test_loss.item():.4f}, Test Accuracy: {accuracy.item()*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c84de403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ba268601",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNHighLevel(nn.Module):\n",
    "    def __init__(self, input_size, num_outputs):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # First convolutional layer, creates 16 output channels/activation maps with 3x3 filters\n",
    "            nn.Conv2d(1,16,3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # Second convolutional layer, creates 32 output channels/activation maps with 3x3 filters based on the 16 from previous layer\n",
    "            nn.Conv2d(16,32,3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32,64,3, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), # scales down by factor 2 so from 28x28 to 14x14\n",
    "            nn.Flatten(), # 14x14x64 = 12544\n",
    "            nn.Linear(12544, num_outputs),\n",
    "            \n",
    "\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f21ed33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "X_train  = X_train.reshape(-1,1,28,28)  # reshape train set\n",
    "model_cnn_high = CNNHighLevel(input_size=28*28, num_outputs=10)\n",
    "logits = model_cnn_high.forward(X_train[:4])  # pass a small batch to see shapes\n",
    "print(\"logits\", logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b55a0e76",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m     loss = loss_fn(logits, y_batch)\n\u001b[32m     15\u001b[39m     optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     optimizer.step()\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#TRAINING BATCHES FOR CNN HIGH LEVEL\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_cnn_high.parameters(), lr=0.1)\n",
    "num_epochs = 10\n",
    "batch_size = 256\n",
    "num_batches = X_train.shape[0] // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(num_batches):\n",
    "        X_batch = X_train[i*batch_size:(i+1)*batch_size]\n",
    "        y_batch = y_train[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        logits = model_cnn_high.forward(X_batch)\n",
    "        loss = loss_fn(logits, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "70ebf506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0530, Test Accuracy: 98.32%\n"
     ]
    }
   ],
   "source": [
    "#EVALUATION FOR CNN HIGH LEVEL\n",
    "with torch.no_grad():\n",
    "    X_test = X_test.reshape(-1,1,28,28)  # reshape test set\n",
    "    test_logits = model_cnn_high.forward(X_test)\n",
    "    test_loss = loss_fn(test_logits, y_test)\n",
    "    predictions = torch.argmax(test_logits, dim=1)\n",
    "    accuracy = (predictions == y_test).float().mean()\n",
    "    print(f\"Test Loss: {test_loss.item():.4f}, Test Accuracy: {accuracy.item()*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "38d9083f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28]) torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "### Fashion MNIST with CNN High Level API\n",
    "# Load Fashion-MNIST from local archive folder\n",
    "archive_root = Path.cwd() / \"archive\"  # Path from working directory to archive\n",
    "\n",
    "FX_train = load_idx_images(archive_root / \"train-images-idx3-ubyte\")\n",
    "Fy_train = load_idx_labels(archive_root / \"train-labels-idx1-ubyte\")\n",
    "FX_test  = load_idx_images(archive_root / \"t10k-images-idx3-ubyte\")\n",
    "Fy_test  = load_idx_labels(archive_root / \"t10k-labels-idx1-ubyte\")\n",
    "\n",
    "# Convert to tensors and normalize\n",
    "FX_train = torch.tensor(FX_train, dtype=torch.float32) / 255.0\n",
    "Fy_train = torch.tensor(Fy_train, dtype=torch.long)\n",
    "FX_test  = torch.tensor(FX_test, dtype=torch.float32) / 255.0\n",
    "Fy_test  = torch.tensor(Fy_test, dtype=torch.long)\n",
    "\n",
    "# Add channel dimension for CNNs\n",
    "FX_train = FX_train.reshape(-1, 1, 28, 28)\n",
    "FX_test  = FX_test.reshape(-1, 1, 28, 28)\n",
    "\n",
    "print(FX_train.shape, Fy_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bf09d132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.3095\n",
      "Epoch 2, Loss: 2.3090\n",
      "Epoch 3, Loss: 2.3095\n",
      "Epoch 4, Loss: 2.3102\n",
      "Epoch 5, Loss: 2.3108\n",
      "Epoch 6, Loss: 2.3112\n",
      "Epoch 7, Loss: 2.3115\n",
      "Epoch 8, Loss: 2.3115\n",
      "Epoch 9, Loss: 2.3116\n",
      "Epoch 10, Loss: 2.3116\n",
      "Epoch 11, Loss: 2.3116\n",
      "Epoch 12, Loss: 2.3116\n",
      "Epoch 13, Loss: 2.3115\n",
      "Epoch 14, Loss: 2.3115\n",
      "Epoch 15, Loss: 2.3115\n"
     ]
    }
   ],
   "source": [
    "model_cnn = CNNHighLevel(input_size=28*28, num_outputs=10)\n",
    "\n",
    "#training loop same as before, just using FX_train and Fy_train instead of X_train and y_train\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_cnn.parameters(), lr=0.1)\n",
    "batch_size = 256\n",
    "num_batches = FX_train.shape[0] // batch_size\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(num_batches):\n",
    "        X_batch = FX_train[i*batch_size:(i+1)*batch_size]\n",
    "        y_batch = Fy_train[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        logits = model_cnn.forward(X_batch)\n",
    "        loss = loss_fn(logits, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030eb1b1",
   "metadata": {},
   "source": [
    "#### LOG\n",
    "With 2 conv layers:\n",
    "conv1 16 filters 3x3\n",
    "conv2 32 filters 3x3\n",
    "Before padding: 86.67%\n",
    "\n",
    "After: 87.90%\n",
    "\n",
    "With 3 conv layers\n",
    "conv1 16 filters 3x3\n",
    "conv2 32 filters 3x3\n",
    "conv3 64 filters 3x3\n",
    "88,07%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2ce63454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3377, Test Accuracy: 88.07%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "with torch.no_grad():\n",
    "    test_logits = model_cnn.forward(FX_test)\n",
    "    test_loss = loss_fn(test_logits, Fy_test)\n",
    "    predictions = torch.argmax(test_logits, dim=1)\n",
    "    accuracy = (predictions == Fy_test).float().mean()\n",
    "    print(f\"Test Loss: {test_loss.item():.4f}, Test Accuracy: {accuracy.item()*100:.2f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acdc47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.9932, -5.1288, -2.9457,  ...,  5.4631,  2.2260,  9.1247],\n",
       "        [ 3.6042, -5.2341, 12.4575,  ..., -9.7423, -2.5794, -4.3412],\n",
       "        [-0.2589, 11.5586, -4.1436,  ..., -7.3697, -3.9229, -7.0524],\n",
       "        ...,\n",
       "        [ 0.5151, -6.1618,  0.0427,  ..., -3.2563,  5.8552, -8.8672],\n",
       "        [-3.4202,  9.3497, -5.1509,  ..., -4.6715, -3.6099, -2.7315],\n",
       "        [-2.1577, -3.2416,  0.2203,  ...,  3.2738,  1.9366, -0.6573]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
