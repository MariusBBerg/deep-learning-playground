{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "863e411f",
   "metadata": {},
   "source": [
    "##### Simple attention first where each token (represented by 32 vector embedding) is \"related\"/look at the average of the previous tokens in the batch. So We have 4,8,32 so 4 batches, 8 tokens in each batch and each token is represented by a 32 long vector embedding. Token number 5, looks at the average vector embedding of the previous 5 (including it self).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06b3e1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "B,T,C = 4,8,32\n",
    "\n",
    "x = torch.randn(B,T,C)\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros(T,T)\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a03bc44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2797, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029],\n",
       "         [0.1773, 0.1773, 0.1076, 0.1076, 0.1076, 0.1076, 0.1076, 0.1076],\n",
       "         [0.1519, 0.1519, 0.1519, 0.1089, 0.1089, 0.1089, 0.1089, 0.1089],\n",
       "         [0.1405, 0.1405, 0.1405, 0.1405, 0.1095, 0.1095, 0.1095, 0.1095],\n",
       "         [0.1341, 0.1341, 0.1341, 0.1341, 0.1341, 0.1098, 0.1098, 0.1098],\n",
       "         [0.1300, 0.1300, 0.1300, 0.1300, 0.1300, 0.1300, 0.1100, 0.1100],\n",
       "         [0.1271, 0.1271, 0.1271, 0.1271, 0.1271, 0.1271, 0.1271, 0.1102],\n",
       "         [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]]),\n",
       " torch.Size([4, 8, 32]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ x\n",
    "\n",
    "wei,out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6423a26b",
   "metadata": {},
   "source": [
    "##### Self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edab351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5075, 0.4925, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2125, 0.5193, 0.2682, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6408, 0.1053, 0.0283, 0.2255, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0704, 0.0789, 0.1102, 0.7165, 0.0241, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2535, 0.1420, 0.0518, 0.2840, 0.1303, 0.1384, 0.0000, 0.0000],\n",
       "         [0.0645, 0.1567, 0.1772, 0.1250, 0.2227, 0.1263, 0.1275, 0.0000],\n",
       "         [0.0200, 0.1377, 0.0445, 0.0886, 0.2897, 0.0489, 0.2079, 0.1627]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1273, 0.8727, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2509, 0.2429, 0.5062, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5969, 0.0856, 0.1509, 0.1666, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3427, 0.0830, 0.3171, 0.0305, 0.2267, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0673, 0.0354, 0.0601, 0.5725, 0.1142, 0.1503, 0.0000, 0.0000],\n",
       "         [0.2210, 0.2613, 0.0540, 0.1997, 0.0736, 0.0573, 0.1331, 0.0000],\n",
       "         [0.0070, 0.0858, 0.0230, 0.0648, 0.0247, 0.0354, 0.3028, 0.4564]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1235, 0.8765, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0081, 0.9367, 0.0553, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1650, 0.0790, 0.7044, 0.0516, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0614, 0.0730, 0.5552, 0.1419, 0.1685, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0945, 0.3716, 0.0298, 0.3025, 0.1884, 0.0133, 0.0000, 0.0000],\n",
       "         [0.0452, 0.1137, 0.0778, 0.5422, 0.0764, 0.0457, 0.0990, 0.0000],\n",
       "         [0.0485, 0.0818, 0.1582, 0.0813, 0.0230, 0.1658, 0.1380, 0.3034]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8921, 0.1079, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0254, 0.9314, 0.0432, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0530, 0.0235, 0.0205, 0.9031, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0250, 0.0267, 0.0089, 0.2622, 0.6772, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2288, 0.0597, 0.0796, 0.1268, 0.4896, 0.0154, 0.0000, 0.0000],\n",
       "         [0.1860, 0.0083, 0.0423, 0.0016, 0.1352, 0.5957, 0.0309, 0.0000],\n",
       "         [0.0388, 0.5396, 0.0429, 0.2545, 0.0457, 0.0167, 0.0217, 0.0401]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "B,T,C = 4,8,32\n",
    "\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "#lets see a single Head perform self-attention\n",
    "head_size=16\n",
    "key = nn.Linear(C,head_size,bias=False)\n",
    "query = nn.Linear(C,head_size,bias=False)\n",
    "value = nn.Linear(C,head_size,bias=False)\n",
    "\n",
    "#When we pass x through key and query, each token in the sequence will get mapped to a key vector and a query vector\n",
    "#of size head_size\n",
    "# So for now there is no communication between different tokens in the sequence.\n",
    "k = key(x)   # (B,T,head_size)\n",
    "q = query(x) # (B,T,head_size)\n",
    "wei = q @ k.transpose(-2,-1) # -2 because we want to transpose the T and head_size dimensions and -1 because we want to keep the batch dimension intact\n",
    "# (B,T,16) @ (B,16,T) gives ut shape (B,T,T)\n",
    "\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "#wei = torch.zeros(T,T)\n",
    "\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "v = value(x) # (B,T,head_size)\n",
    "out = wei @ v\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe69253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Self-Attention (single head) — intuition-first version\n",
    "# -------------------------------------------------------\n",
    "\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# x[b, t] is the embedding vector of token t in batch element b\n",
    "# Shape intuition:\n",
    "# - B: independent examples (never communicate with each other)\n",
    "# - T: tokens in a sequence\n",
    "# - C: embedding / feature dimension per token\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Define one attention head\n",
    "# -------------------------------------------------------\n",
    "\n",
    "head_size = 16\n",
    "\n",
    "key   = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# These are learned linear projections applied *independently* to each token.\n",
    "# No communication yet — each token is processed on its own.\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Step 1: produce Queries, Keys, Values\n",
    "# -------------------------------------------------------\n",
    "\n",
    "k = key(x)    # (B, T, head_size)\n",
    "q = query(x)  # (B, T, head_size)\n",
    "v = value(x)  # (B, T, head_size)\n",
    "\n",
    "# Meaning of these vectors:\n",
    "#\n",
    "# q[b,t] — QUERY\n",
    "#   \"What is token t looking for?\"\n",
    "#   Think: a question vector.\n",
    "#\n",
    "# k[b,t] — KEY\n",
    "#   \"What kind of information does token t offer?\"\n",
    "#   Think: a label / description vector.\n",
    "#\n",
    "# v[b,t] — VALUE\n",
    "#   \"What information should token t contribute if attended to?\"\n",
    "#   Think: the actual content to be copied / aggregated.\n",
    "#   - NOTE: v is distinct from x (the original token embedding). We can think of x as private/raw information and v as public/communicable information.\n",
    "#\n",
    "# IMPORTANT:\n",
    "# - q, k, v are NOT probabilities\n",
    "# - their *direction* matters, not their absolute values\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Step 2: raw attention scores (similarity)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# We want, for each batch b, ALL pairwise dot-products between:\n",
    "#   - each query at position i: q[b,i]  (shape head_size)\n",
    "#   - each key   at position j: k[b,j]  (shape head_size)\n",
    "#\n",
    "# q has shape (B, T, head). To get a (T×T) matrix of scores, we need:\n",
    "#   scores[b,i,j] = dot(q[b,i], k[b,j])\n",
    "#\n",
    "# k is (B, T, head), so we transpose its last two dims -> (B, head, T),\n",
    "# making the matmul:\n",
    "#   (B, T, head) @ (B, head, T) -> (B, T, T)\n",
    "#\n",
    "# Why transpose(-2,-1)?  -1 means “last dim”, -2 means “second-to-last dim”.\n",
    "# This swaps (T, head) -> (head, T) while keeping batch B unchanged.\n",
    "\n",
    "wei = (q @ k.transpose(-2, -1)) * head_size**-0.5\n",
    "\n",
    "# Shape: (B, T, T)\n",
    "\n",
    "# wei[b, i, j] = dot(q[b,i], k[b,j]) / sqrt(head_size)\n",
    "#\n",
    "# Interpretation:\n",
    "# - HIGH value  → token i finds token j very relevant\n",
    "# - ~0          → token j is mostly irrelevant to token i\n",
    "# - NEGATIVE    → token j is actively unhelpful to token i\n",
    "#\n",
    "# For a fixed (b, i):\n",
    "#   wei[b,i,:] answers:\n",
    "#   \"Which other tokens should token i pay attention to?\"\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Step 3: causal (triangular) masking — decoder attention\n",
    "# -------------------------------------------------------\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "\n",
    "# This enforces AUTOREGRESSIVE behavior:\n",
    "# - token i may attend to tokens j <= i\n",
    "# - token i may NOT attend to tokens j > i (the future)\n",
    "#\n",
    "# Setting scores to -inf ensures:\n",
    "#   softmax → probability 0\n",
    "#\n",
    "# Without this line:\n",
    "# - all tokens can talk to all tokens\n",
    "# - this becomes *encoder self-attention*\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Step 4: convert scores to probabilities\n",
    "# -------------------------------------------------------\n",
    "\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "# Now:\n",
    "# - wei[b,i,:] is a probability distribution\n",
    "# - sum_j wei[b,i,j] = 1\n",
    "#\n",
    "# Meaning:\n",
    "# - HIGH probability → strong information flow from token j to token i\n",
    "# - LOW probability  → weak or negligible influence\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Step 5: weighted aggregation of values (communication)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "out = wei @ v   # (B, T, head_size)\n",
    "\n",
    "# out[b,i] = sum_j ( wei[b,i,j] * v[b,j] )\n",
    "#\n",
    "# This is the core of attention:\n",
    "# - token i *collects* information from other tokens\n",
    "# - each token contributes proportionally to its attention weight\n",
    "#\n",
    "# Intuition:\n",
    "# - If attention is very sharp → out[b,i] ≈ v[b,j*]\n",
    "# - If attention is diffuse → out[b,i] is a blend of many tokens\n",
    "\n",
    "# This out tensor is the result of ONE attention head.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5871b59c",
   "metadata": {},
   "source": [
    "#### Concrete Example: Q @ K.T with batch dimension\n",
    "\n",
    "Let's use a tiny example: **B=2 batches, T=3 tokens, head_size=4**\n",
    "\n",
    "After linear projections, we have:\n",
    "\n",
    "```python\n",
    "# Shape: (B=2, T=3, head_size=4)\n",
    "q = torch.tensor([\n",
    "  # Batch 0:\n",
    "  [\n",
    "    [1, 0, 1, 0],  # token 0 is \"looking for\" this pattern\n",
    "    [0, 1, 0, 1],  # token 1 is \"looking for\" this pattern  \n",
    "    [1, 1, 0, 0],  # token 2 is \"looking for\" this pattern\n",
    "  ],\n",
    "  # Batch 1:\n",
    "  [\n",
    "    [2, 0, 0, 1],  # token 0 in different example\n",
    "    [0, 2, 1, 0],  # token 1 in different example\n",
    "    [1, 0, 1, 1],  # token 2 in different example\n",
    "  ]\n",
    "])  # shape: (2, 3, 4)\n",
    "\n",
    "k = torch.tensor([\n",
    "  # Batch 0:\n",
    "  [\n",
    "    [1, 0, 0, 1],  # token 0 \"offers\" this pattern\n",
    "    [0, 1, 1, 0],  # token 1 \"offers\" this pattern\n",
    "    [1, 1, 0, 0],  # token 2 \"offers\" this pattern\n",
    "  ],\n",
    "  # Batch 1:\n",
    "  [\n",
    "    [2, 0, 1, 0],  # token 0 offers different pattern\n",
    "    [0, 2, 0, 1],  # token 1 offers different pattern\n",
    "    [1, 0, 1, 1],  # token 2 offers different pattern\n",
    "  ]\n",
    "])  # shape: (2, 3, 4)\n",
    "```\n",
    "\n",
    "**Step 1: Transpose k**\n",
    "\n",
    "`k.transpose(-2, -1)` swaps the last two dimensions: (B, T, head) → (B, head, T)\n",
    "\n",
    "```python\n",
    "# k.transpose(-2, -1) has shape: (2, 4, 3)\n",
    "# \n",
    "# For batch 0, k[0].T becomes:\n",
    "# [\n",
    "#   [1, 0, 1],  ← first dimension of all 3 tokens\n",
    "#   [0, 1, 1],  ← second dimension of all 3 tokens\n",
    "#   [0, 1, 0],  ← third dimension\n",
    "#   [1, 0, 0],  ← fourth dimension\n",
    "# ]\n",
    "```\n",
    "\n",
    "**Step 2: Matrix multiply q @ k.T**\n",
    "\n",
    "```python\n",
    "wei = q @ k.transpose(-2, -1)\n",
    "# (B=2, T=3, head=4) @ (B=2, head=4, T=3) → (B=2, T=3, T=3)\n",
    "```\n",
    "\n",
    "Let's compute **batch 0** in detail. We're doing:\n",
    "\n",
    "```\n",
    "       k[0].T\n",
    "         ↓\n",
    "    [1  0  1]\n",
    "    [0  1  1]\n",
    "    [0  1  0]\n",
    "    [1  0  0]\n",
    "\n",
    "q[0] @ k[0].T:\n",
    "\n",
    "[1 0 1 0]     [1  0  1]     [?  ?  ?]\n",
    "[0 1 0 1]  @  [0  1  1]  =  [?  ?  ?]\n",
    "[1 1 0 0]     [0  1  0]     [?  ?  ?]\n",
    "              [1  0  0]\n",
    "\n",
    "(3×4)    @    (4×3)    =    (3×3)\n",
    "```\n",
    "\n",
    "**Row 0 of q[0] × all columns of k[0].T:**\n",
    "\n",
    "```\n",
    "[1 0 1 0] · [1]   = 1×1 + 0×0 + 1×0 + 0×1 = 2\n",
    "            [0]\n",
    "            [0]\n",
    "            [1]\n",
    "\n",
    "[1 0 1 0] · [0]   = 1×0 + 0×1 + 1×1 + 0×0 = 1\n",
    "            [1]\n",
    "            [1]\n",
    "            [0]\n",
    "\n",
    "[1 0 1 0] · [1]   = 1×1 + 0×1 + 1×0 + 0×0 = 1\n",
    "            [1]\n",
    "            [0]\n",
    "            [0]\n",
    "```\n",
    "Result: **first row = [2, 1, 1]**\n",
    "\n",
    "\n",
    "**Row 1 of q[0] × all columns of k[0].T:**\n",
    "\n",
    "```\n",
    "[0 1 0 1] · [1]   = 0×1 + 1×0 + 0×0 + 1×1 = 1\n",
    "            [0]\n",
    "            [0]\n",
    "            [1]\n",
    "\n",
    "[0 1 0 1] · [0]   = 0×0 + 1×1 + 0×1 + 1×0 = 1\n",
    "            [1]\n",
    "            [1]\n",
    "            [0]\n",
    "\n",
    "[0 1 0 1] · [1]   = 0×1 + 1×1 + 0×0 + 1×0 = 1\n",
    "            [1]\n",
    "            [0]\n",
    "            [0]\n",
    "```\n",
    "Result: **second row = [1, 1, 1]**\n",
    "\n",
    "\n",
    "**Row 2 of q[0] × all columns of k[0].T:**\n",
    "\n",
    "```\n",
    "[1 1 0 0] · [1]   = 1×1 + 1×0 + 0×0 + 0×1 = 1\n",
    "            [0]\n",
    "            [0]\n",
    "            [1]\n",
    "\n",
    "[1 1 0 0] · [0]   = 1×0 + 1×1 + 0×1 + 0×0 = 1\n",
    "            [1]\n",
    "            [1]\n",
    "            [0]\n",
    "\n",
    "[1 1 0 0] · [1]   = 1×1 + 1×1 + 0×0 + 0×0 = 2\n",
    "            [1]\n",
    "            [0]\n",
    "            [0]\n",
    "```\n",
    "Result: **third row = [1, 1, 2]**\n",
    "\n",
    "\n",
    "**Final result for batch 0:**\n",
    "```python\n",
    "wei[0] = [\n",
    "  [2, 1, 1],  # token 0's raw scores for attending to [tok0, tok1, tok2]\n",
    "  [1, 1, 1],  # token 1's raw scores for attending to [tok0, tok1, tok2]\n",
    "  [1, 1, 2],  # token 2's raw scores for attending to [tok0, tok1, tok2]\n",
    "]\n",
    "```\n",
    "\n",
    "For **batch 1** (computed independently with same process):\n",
    "```python\n",
    "wei[1] = [\n",
    "  [5, 2, 3],  # batch 1, token 0's scores\n",
    "  [1, 4, 1],  # batch 1, token 1's scores\n",
    "  [4, 1, 3],  # batch 1, token 2's scores\n",
    "]\n",
    "```\n",
    "\n",
    "**Why does this make sense?**\n",
    "\n",
    "The matmul efficiently computes **all pairwise similarities** within each batch:\n",
    "- High score → query and key vectors are aligned → high attention\n",
    "- Low/negative score → vectors are orthogonal/opposite → low attention\n",
    "\n",
    "**Key insight:** Batches never interact! `wei[0]` only uses `q[0]` and `k[0]`. Each batch processes its own sequence independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1954e1",
   "metadata": {},
   "source": [
    "#### Attention as a communication mechanism\n",
    "\n",
    "Attention is a communication mechanism.\n",
    "\n",
    "You can think of it as:\n",
    "\n",
    "tokens = nodes in a directed graph\n",
    "\n",
    "attention weights = data-dependent edge strengths\n",
    "\n",
    "output = weighted aggregation of information from connected nodes\n",
    "\n",
    "Each token decides where to send its attention, and how much information to pull back.\n",
    "\n",
    "\n",
    "#### No notion of space\n",
    "\n",
    "Attention operates on a set of vectors, not on space or time.\n",
    "\n",
    "There is:\n",
    "- no inherent order\n",
    "- no notion of distance\n",
    "- no notion of “next to”\n",
    "\n",
    "That’s why positional encoding is required in Transformers:\n",
    "- position is injected externally\n",
    "- not handled by attention itself\n",
    "(This simple code does not include positional encoding.)\n",
    "\n",
    "\n",
    "\n",
    "#### Encoder vs Decoder attention\n",
    "\n",
    "Encoder self-attention:\n",
    "- remove the triangular mask\n",
    "- all tokens can attend to all tokens\n",
    "\n",
    "Decoder self-attention (this code):\n",
    "- uses triangular masking\n",
    "- enforces causality\n",
    "- used in autoregressive models (e.g. language modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cd0dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_emb = 32\n",
    "\n",
    "#input is (B,T,n_emb)\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key   = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_emb, head_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        k = self.key(x)    # (B, T, head_size)\n",
    "        q = self.query(x)  # (B, T, head_size)\n",
    "        v = self.value(x)  # (B, T, head_size)\n",
    "\n",
    "        # compute attention scores\n",
    "        wei = (q @ k.transpose(-2, -1)) * head_size**-0.5  # (B, T, T) same as above\n",
    "        tril = torch.tril(torch.ones(T, T))\n",
    "        wei = wei.masked_fill(tril == 0, float(\"-inf\")) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "\n",
    "        out = wei @ v   # (B,T,T) @ (B,T,C) -> (B, T, C/head_size)\n",
    "        return out\n",
    "\n",
    "#output is (B,T,head_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
